<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>physics | Steeven Janny</title>
    <link>https://steevenjanny.github.io/tag/physics/</link>
      <atom:link href="https://steevenjanny.github.io/tag/physics/index.xml" rel="self" type="application/rss+xml" />
    <description>physics</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 26 Jan 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://steevenjanny.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>physics</title>
      <link>https://steevenjanny.github.io/tag/physics/</link>
    </image>
    
    <item>
      <title>Filtered-CoPhy: Unsupervised Learning of Counterfactual Physics in Pixel Space</title>
      <link>https://steevenjanny.github.io/post/filteredcophy/</link>
      <pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://steevenjanny.github.io/post/filteredcophy/</guid>
      <description>&lt;p&gt;Learning causal relationships in high-dimensional data (images, videos) is a hard task, as they are often defined on low dimensional manifolds and must be extracted from complex signals dominated by appearance, lighting, textures and also spurious correlations in the data. We present a method for learning counterfactual reasoning of physical processes in pixel space, which requires the prediction of the impact of interventions on initial conditions. Going beyond the identification of structural relationships, we deal with the challenging problem of forecasting raw video over long horizons. Our method does not require the knowledge or supervision of any ground truth positions or other object or scene properties. Our model learns and acts on a suitable hybrid latent representation based on a combination of dense features, sets of 2D keypoints and an additional latent vector per keypoint. We show that this better captures the dynamics of physical processes than purely dense or sparse representations. We introduce a new challenging and carefully designed counterfactual benchmark for predictions in pixel space and outperform strong baselines in physics-inspired ML and video prediction.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>EAGLE: Large-scale Learning of Turbulent Fluid Dynamics with Mesh Transformers</title>
      <link>https://steevenjanny.github.io/post/eagledataset/</link>
      <pubDate>Thu, 26 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://steevenjanny.github.io/post/eagledataset/</guid>
      <description>&lt;p&gt;Estimating fluid dynamics is classically done through the simulation and integration of numerical models solving the
Navier-Stokes equations, which is computationally complex and time-consuming even on high-end hardware. This is a
notoriously hard problem to solve, which has recently been addressed with machine learning, in particular graph neural
networks (GNN) and variants trained and evaluated on datasets of static objects in static scenes with fixed geometry. We
attempt to go beyond existing work in complexity and introduce a new model, method and benchmark. We propose EAGLE, a
large-scale dataset of âˆ¼1.1 million 2D meshes resulting from simulations of unsteady fluid dynamics caused by a moving
flow source interacting with nonlinear scene structure, comprised of 600 different scenes of three different types. To
perform future forecasting of pressure and velocity on the challenging EAGLE dataset, we introduce a new mesh
transformer. It leverages node clustering, graph pooling and global attention to learn long-range dependencies between
spatially distant data points without needing a large number of iterations, as existing GNN methods do. We show that our
transformer outperforms state-of-the-art performance on, both, existing synthetic and real datasets and on EAGLE.
Finally, we highlight that our approach learns to attend to airflow, integrating complex information in a single
iteration.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
